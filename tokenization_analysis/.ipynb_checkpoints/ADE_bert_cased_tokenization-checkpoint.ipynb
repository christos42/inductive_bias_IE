{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AlbertTokenizer, AlbertModel, AutoModelForPreTraining\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('../data/ADE/raw/ade_split_0_test.json')\n",
    "data_test = json.load(f)\n",
    "\n",
    "f = open('../data/ADE/raw/ade_split_0_train.json')\n",
    "data_all = json.load(f)\n",
    "\n",
    "data_all.extend(data_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = []\n",
    "for s in data_all:\n",
    "    sent_len.append(len(s['tokens']))\n",
    "\n",
    "print('Average sentence length: {}' .format(np.round(np.mean(sent_len), 2)))\n",
    "print('Max sentence length: {}' .format(np.max(sent_len)))\n",
    "print('Min sentence length: {}' .format(np.min(sent_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenized version:')\n",
    "tokenized_sent_len = []\n",
    "for s in data_all:\n",
    "    sent_str = ' '.join(s['tokens'])\n",
    "    tokenized_sent_len.append(len(tokenizer.tokenize(sent_str)))\n",
    "\n",
    "print('Average sentence length: {}' .format(np.round(np.mean(tokenized_sent_len), 2)))\n",
    "print('Max sentence length: {}' .format(np.max(tokenized_sent_len)))\n",
    "print('Min sentence length: {}' .format(np.min(tokenized_sent_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {'Drug': [],\n",
    "            'Adverse-Effect': []}\n",
    "true_entities_indexes = []\n",
    "for s in data_all:\n",
    "    for en in s['entities']:\n",
    "        entities[en['type']].append([t for t in s['tokens'][en['start']:en['end']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities_flattened = []\n",
    "for en in entities['Drug']:\n",
    "    for s_en in en:\n",
    "        all_entities_flattened.append(s_en)\n",
    "for en in entities['Adverse-Effect']:\n",
    "    for s_en in en:\n",
    "        all_entities_flattened.append(s_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['Outside'] = []\n",
    "for s in data_all:\n",
    "    for t in s['tokens']:\n",
    "        if t not in all_entities_flattened:\n",
    "            entities['Outside'].append([t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total DRUG entities: {}' .format(len(entities['Drug'])))\n",
    "print('Total Adverse-Effect entities: {}' .format(len(entities['Adverse-Effect'])))\n",
    "print('Total Outside entities: {}' .format(len(entities['Outside'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_entities = {'Drug': [],\n",
    "                   'Adverse-Effect': [],\n",
    "                   'Outside': []}\n",
    "for k in entities.keys():\n",
    "    for en in entities[k]:\n",
    "        if en not in unique_entities[k]:\n",
    "            unique_entities[k].append(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total unique DRUG entities: {}' .format(len(unique_entities['Drug'])))\n",
    "print('Total unique Adverse-Effect entities: {}' .format(len(unique_entities['Adverse-Effect'])))\n",
    "print('Total unique Outside entities: {}' .format(len(unique_entities['Outside'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_entities = {'Drug': [],\n",
    "                'Adverse-Effect': [],\n",
    "                'Outside': []}\n",
    "for k in unique_entities.keys():\n",
    "    for en in unique_entities[k]:\n",
    "        len_entities[k].append(len(en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average length of the Drug entity: {}' .format(np.round(np.mean(len_entities['Drug']), 2)))\n",
    "print('Average length of the Adverse-Effect entity: {}' .format(np.round(np.mean(len_entities['Adverse-Effect']), 2)))\n",
    "print('Average length of the Outside entity: {}' .format(np.round(np.mean(len_entities['Outside']), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_unique_entities = {'Drug': [],\n",
    "                             'Adverse-Effect': [],\n",
    "                             'Outside': []}\n",
    "for k in unique_entities.keys():\n",
    "    for en in unique_entities[k]:\n",
    "        en_str = ' '.join(en)\n",
    "        tokenized_unique_entities[k].append(tokenizer.tokenize(en_str))\n",
    "\n",
    "len_tokenized_entities = {'Drug': [],\n",
    "                          'Adverse-Effect': [],\n",
    "                          'Outside': []}\n",
    "for k in tokenized_unique_entities.keys():\n",
    "    for en in tokenized_unique_entities[k]:\n",
    "        len_tokenized_entities[k].append(len(en))\n",
    "\n",
    "print('Tokenized version:')\n",
    "print('Average length of Drug entity: {}' .format(np.round(np.mean(len_tokenized_entities['Drug']), 2)))\n",
    "print('Average length of Adverse-Effect entity: {}' .format(np.round(np.mean(len_tokenized_entities['Adverse-Effect']), 2)))\n",
    "print('Average length of Outside entity: {}' .format(np.round(np.mean(len_tokenized_entities['Outside']), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for k in tokenizer.get_vocab().keys():\n",
    "    vocabulary.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for k in tokenizer.get_vocab().keys():\n",
    "    if k[:2] == '##':\n",
    "        vocabulary.append(k[2:])\n",
    "    else:\n",
    "        vocabulary.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_unique_entities_words = {'Drug': [],\n",
    "                                   'Adverse-Effect': [],\n",
    "                                   'Outside': []}\n",
    "for k in unique_entities.keys():\n",
    "    for en in unique_entities[k]:\n",
    "        for s_en in en:\n",
    "            if s_en not in flattened_unique_entities_words[k]:\n",
    "                flattened_unique_entities_words[k].append(s_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_flattened_unique_entities_words = {'Drug': [],\n",
    "                                             'Adverse-Effect': [],\n",
    "                                             'Outside': []}\n",
    "for k in flattened_unique_entities_words.keys():\n",
    "    for w in flattened_unique_entities_words[k]:\n",
    "        tokenized_flattened_unique_entities_words[k].append(tokenizer.tokenize(w))\n",
    "\n",
    "len_tokenized_flattened_unique_entities_words = {'Drug': [],\n",
    "                                                 'Adverse-Effect': [],\n",
    "                                                 'Outside': []}\n",
    "for k in tokenized_flattened_unique_entities_words.keys():\n",
    "    for w in tokenized_flattened_unique_entities_words[k]:\n",
    "        len_tokenized_flattened_unique_entities_words[k].append(len(w))\n",
    "\n",
    "print('Tokenized version:')\n",
    "print('Average length of Drug word: {}' .format(np.round(np.mean(len_tokenized_flattened_unique_entities_words['Drug']), 2)))\n",
    "print('Average length of Adverse-Effect word: {}' .format(np.round(np.mean(len_tokenized_flattened_unique_entities_words['Adverse-Effect']), 2)))\n",
    "print('Average length of Outside word: {}' .format(np.round(np.mean(len_tokenized_flattened_unique_entities_words['Outside']), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total unique words, part of DRUG entities: {}' .format(len(flattened_unique_entities_words['Drug'])))\n",
    "print('Total unique words, part of Adverse-Effect entities: {}' .format(len(flattened_unique_entities_words['Adverse-Effect'])))\n",
    "print('Total unique Outside words: {}' .format(len(flattened_unique_entities_words['Outside'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_words = {'Drug': [],\n",
    "             'Adverse-Effect': [],\n",
    "             'Outside': []}\n",
    "for k in flattened_unique_entities_words.keys():\n",
    "    for w in flattened_unique_entities_words[k]:\n",
    "        if w not in vocabulary:\n",
    "            oov_words[k].append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of OOV Drug words (words to be split in word-pieces): {}%' .format(100*np.round(len(oov_words['Drug'])/len(flattened_unique_entities_words['Drug']), 3)))\n",
    "print('Percentage of OOV Adverse-Effect words (words to be split in word-pieces): {}%' .format(100*np.round(len(oov_words['Adverse-Effect'])/len(flattened_unique_entities_words['Adverse-Effect']), 3)))\n",
    "print('Percentage of OOV Outside words (words to be split in word-pieces): {}%' .format(100*np.round(len(oov_words['Outside'])/len(flattened_unique_entities_words['Outside']), 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
