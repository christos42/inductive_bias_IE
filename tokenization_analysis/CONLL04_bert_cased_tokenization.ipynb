{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AlbertTokenizer, AlbertModel, AutoModelForPreTraining\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('../data/CONLL04/dev_triples.json')\n",
    "data_dev = json.load(f)\n",
    "\n",
    "f = open('../data/CONLL04/test_triples.json')\n",
    "data_test = json.load(f)\n",
    "\n",
    "f = open('../data/CONLL04/train_triples.json')\n",
    "data_train = json.load(f)\n",
    "\n",
    "data_all = []\n",
    "data_all.extend(data_dev)\n",
    "data_all.extend(data_test)\n",
    "data_all.extend(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = []\n",
    "for s in data_all:\n",
    "    sent_len.append(len(s['tokens']))\n",
    "\n",
    "print('Average sentence length: {}' .format(np.round(np.mean(sent_len), 2)))\n",
    "print('Max sentence length: {}' .format(np.max(sent_len)))\n",
    "print('Min sentence length: {}' .format(np.min(sent_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenized version:')\n",
    "tokenized_sent_len = []\n",
    "for s in data_all:\n",
    "    sent_str = ' '.join(s['tokens'])\n",
    "    tokenized_sent_len.append(len(tokenizer.tokenize(sent_str)))\n",
    "\n",
    "print('Average sentence length: {}' .format(np.round(np.mean(tokenized_sent_len), 2)))\n",
    "print('Max sentence length: {}' .format(np.max(tokenized_sent_len)))\n",
    "print('Min sentence length: {}' .format(np.min(tokenized_sent_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {'Peop': [],\n",
    "            'Loc': [],\n",
    "            'Other': [],\n",
    "            'Org': []}\n",
    "\n",
    "for s in data_all:\n",
    "    for en in s['entities']:\n",
    "        entities[en['type']].append([t for t in s['tokens'][en['start']:en['end']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities_flattened = []\n",
    "for k in entities.keys():\n",
    "    for en in entities[k]:\n",
    "        for s_en in en:\n",
    "            all_entities_flattened.append(s_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['Outside'] = []\n",
    "for s in data_all:\n",
    "    for t in s['tokens']:\n",
    "        if t not in all_entities_flattened:\n",
    "            entities['Outside'].append([t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Peop entities: {}' .format(len(entities['Peop'])))\n",
    "print('Total Loc entities: {}' .format(len(entities['Loc'])))\n",
    "print('Total Other entities: {}' .format(len(entities['Other'])))\n",
    "print('Total Org entities: {}' .format(len(entities['Org'])))\n",
    "print('Total Outside entities: {}' .format(len(entities['Outside'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_entities = {'Peop': [],\n",
    "                   'Loc': [],\n",
    "                   'Other': [],\n",
    "                   'Org': [],\n",
    "                   'Outside': []}\n",
    "for k in entities.keys():\n",
    "    for en in entities[k]:\n",
    "        if en not in unique_entities[k]:\n",
    "            unique_entities[k].append(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Peop entities: {}' .format(len(unique_entities['Peop'])))\n",
    "print('Total Loc entities: {}' .format(len(unique_entities['Loc'])))\n",
    "print('Total Other entities: {}' .format(len(unique_entities['Other'])))\n",
    "print('Total Org entities: {}' .format(len(unique_entities['Org'])))\n",
    "print('Total Outside entities: {}' .format(len(unique_entities['Outside'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_entities = {'Peop': [],\n",
    "                'Loc': [],\n",
    "                'Other': [],\n",
    "                'Org': [],\n",
    "                'Outside': []}\n",
    "for k in unique_entities.keys():\n",
    "    for en in unique_entities[k]:\n",
    "        len_entities[k].append(len(en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average length of the Peop entity: {}' .format(np.round(np.mean(len_entities['Peop']), 2)))\n",
    "print('Average length of the Loc entity: {}' .format(np.round(np.mean(len_entities['Loc']), 2)))\n",
    "print('Average length of the Other entity: {}' .format(np.round(np.mean(len_entities['Other']), 2)))\n",
    "print('Average length of the Org entity: {}' .format(np.round(np.mean(len_entities['Org']), 2)))\n",
    "print('Average length of the Outside entity: {}' .format(np.round(np.mean(len_entities['Outside']), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_unique_entities = {'Peop': [],\n",
    "                             'Loc': [],\n",
    "                             'Other': [],\n",
    "                             'Org': [],\n",
    "                             'Outside': []}\n",
    "for k in unique_entities.keys():\n",
    "    for en in unique_entities[k]:\n",
    "        en_str = ' '.join(en)\n",
    "        tokenized_unique_entities[k].append(tokenizer.tokenize(en_str))\n",
    "\n",
    "len_tokenized_entities = {'Peop': [],\n",
    "                          'Loc': [],\n",
    "                          'Other': [],\n",
    "                          'Org': [],\n",
    "                          'Outside': []}\n",
    "for k in tokenized_unique_entities.keys():\n",
    "    for en in tokenized_unique_entities[k]:\n",
    "        len_tokenized_entities[k].append(len(en))\n",
    "\n",
    "print('Tokenized version:')\n",
    "print('Average length of the Peop entity: {}' .format(np.round(np.mean(len_tokenized_entities['Peop']), 2)))\n",
    "print('Average length of the Loc entity: {}' .format(np.round(np.mean(len_tokenized_entities['Loc']), 2)))\n",
    "print('Average length of the Other entity: {}' .format(np.round(np.mean(len_tokenized_entities['Other']), 2)))\n",
    "print('Average length of the Org entity: {}' .format(np.round(np.mean(len_tokenized_entities['Org']), 2)))\n",
    "print('Average length of the Outside entity: {}' .format(np.round(np.mean(len_tokenized_entities['Outside']), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for k in tokenizer.get_vocab().keys():\n",
    "    vocabulary.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_unique_entities_words = {'Peop': [],\n",
    "                                   'Loc': [],\n",
    "                                   'Other': [],\n",
    "                                   'Org': [],\n",
    "                                   'Outside': []}\n",
    "for k in unique_entities.keys():\n",
    "    for en in unique_entities[k]:\n",
    "        for s_en in en:\n",
    "            if s_en not in flattened_unique_entities_words[k]:\n",
    "                flattened_unique_entities_words[k].append(s_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total unique words, part of Peop entities: {}' .format(len(flattened_unique_entities_words['Peop'])))\n",
    "print('Total unique words, part of Loc entities: {}' .format(len(flattened_unique_entities_words['Loc'])))\n",
    "print('Total unique words, part of Other entities: {}' .format(len(flattened_unique_entities_words['Other'])))\n",
    "print('Total unique words, part of Org entities: {}' .format(len(flattened_unique_entities_words['Org'])))\n",
    "print('Total unique Outside words: {}' .format(len(flattened_unique_entities_words['Outside'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_flattened_unique_entities_words = {'Peop': [],\n",
    "                                             'Loc': [],\n",
    "                                             'Other': [],\n",
    "                                             'Org': [],\n",
    "                                             'Outside': []}\n",
    "for k in flattened_unique_entities_words.keys():\n",
    "    for w in flattened_unique_entities_words[k]:\n",
    "        tokenized_flattened_unique_entities_words[k].append(tokenizer.tokenize(w))\n",
    "\n",
    "len_tokenized_flattened_unique_entities_words = {'Peop': [],\n",
    "                                                 'Loc': [],\n",
    "                                                 'Other': [],\n",
    "                                                 'Org': [],\n",
    "                                                 'Outside': []}\n",
    "for k in tokenized_flattened_unique_entities_words.keys():\n",
    "    for w in tokenized_flattened_unique_entities_words[k]:\n",
    "        len_tokenized_flattened_unique_entities_words[k].append(len(w))\n",
    "\n",
    "print('Tokenized version:')\n",
    "print('Average length of Peop word: {}' .format(np.round(np.mean(len_tokenized_flattened_unique_entities_words['Peop']), 2)))\n",
    "print('Average length of Loc word: {}' .format(np.round(np.mean(len_tokenized_flattened_unique_entities_words['Loc']), 2)))\n",
    "print('Average length of Other word: {}' .format(np.round(np.mean(len_tokenized_flattened_unique_entities_words['Other']), 2)))\n",
    "print('Average length of Org word: {}' .format(np.round(np.mean(len_tokenized_flattened_unique_entities_words['Org']), 2)))\n",
    "print('Average length of Outside word: {}' .format(np.round(np.mean(len_tokenized_flattened_unique_entities_words['Outside']), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_words = {'Peop': [],\n",
    "             'Loc': [],\n",
    "             'Other': [],\n",
    "             'Org': [],\n",
    "             'Outside': []}\n",
    "for k in flattened_unique_entities_words.keys():\n",
    "    for w in flattened_unique_entities_words[k]:\n",
    "        if w not in vocabulary:\n",
    "            oov_words[k].append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of OOV Peop words (words to be split in word-pieces): {}%' .format(100*np.round(len(oov_words['Peop'])/len(flattened_unique_entities_words['Peop']), 3)))\n",
    "print('Percentage of OOV Loc words (words to be split in word-pieces): {}%' .format(100*np.round(len(oov_words['Loc'])/len(flattened_unique_entities_words['Loc']), 3)))\n",
    "print('Percentage of OOV Other words (words to be split in word-pieces): {}%' .format(100*np.round(len(oov_words['Other'])/len(flattened_unique_entities_words['Other']), 3)))\n",
    "print('Percentage of OOV Org words (words to be split in word-pieces): {}%' .format(100*np.round(len(oov_words['Org'])/len(flattened_unique_entities_words['Org']), 3)))\n",
    "print('Percentage of OOV Outside words (words to be split in word-pieces): {}%' .format(100*np.round(len(oov_words['Outside'])/len(flattened_unique_entities_words['Outside']), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
